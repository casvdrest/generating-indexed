In this section, we discuss some of the existing literature that is relevant in the 
domain of generating test data for property based testing. We take a look at some 
existing testing libraries, techniques for generation of constrained test data, and a 
few type universes beyond those we used that aim to describe (at least a subset of) 
indexed datatypes. 

\section{Libraries for Property Based Testing}

  \textit{Property Based Testing} aims to assert properties that universally hold for 
  our programs by parameterizing tests over values and checking them against a 
  collection of test values. Libraries for property based testing often include some 
  kind of mechanism to automatically generate collections of test values. Existing 
  tools take different approaches towards generation of test data: \textit{QuickCheck} 
  \cite{claessen2011quickcheck} randomly generates values within the test domain, 
  while \textit{SmallCheck} \cite{runciman2008smallcheck} and \textit{LeanCheck} \cite
  {matela2017tools} exhaustively enumerate all values in the test domain up to a 
  certain point. There exist many libraries for property based testing. For brevity we 
  constrain ourselves here to those that are relevant in the domain of functional 
  programming and/or haskell. 

\subsection{QuickCheck} 

  Published in 2000 by Claessen \& Hughes \cite{claessen2011quickcheck}, QuickCheck 
  implements property based testing for Haskell. Test values are generated by sampling randomly from the domain of test values. QuickCheck supplies 
  the typeclass \texttt{Arbitrary}, whose instances are those types for which random 
  values can be generated. A property of type |a -> Bool| can be tested if |a| is an 
  instance of \texttt{Arbitrary}. Instances for most common Haskell types are supplied 
  by the library. If a property fails on a testcase, QuickCheck supplies a 
  counterexample. Consider the following faulty definition of |reverse|: 

\begin{myhaskell}
\begin{code}
reverse :: Eq a => [a] -> [a]
reverse []      =  []
reverse (x:xs)  =  nub ((reverse xs) ++ [x, x])
\end{code}
\end{myhaskell}

  If we now test our function by calling |quickChec 
  reverse_preserves_length|, we get the following output: 

\begin{verbatim}
Test.QuickCheck> quickCheck reverse_preserves_length 
*** Failed! Falsifiable (after 8 tests and 2 shrinks):    
[7,7]
\end{verbatim}

  We see that a counterexample was found after 8 tests \textit{and 2 shrinks}. Due to 
  the random nature of the tested values, the counterexamples that falsify a property 
  are almost never minimal counterexamples. QuickCheck takes a counterexample and 
  applies some function that produces a collection of values that are smaller than the 
  original counterexample, and attempts to falsify the property using one of the 
  smaller values. By repeatedly \textit{Shrinking} a counterexample, QuickCheck is 
  able to find much smaller counterexamples, which are in general of much more use to 
  the programmer. 

  Perhaps somewhat surprising is that QuickCheck is also able randomly generate values 
  for function types by modifying the seed of the random generator (which is used to 
  generate the function's output) based on it's input. 

\subsection{(Lazy) SmallCheck} 

  Contrary to QuickCheck, SmallCheck \cite{runciman2008smallcheck} takes an \textit
  {enumerative} approach to the generation of test data. While the approach to 
  formulation and testing of properties is largely similar to QuickCheck's, test 
  values are not generated at random, but rather exhaustively enumerated up to a 
  certain \textit{depth}. Zero-arity constructors have depth $0$, while the depth of 
  any positive arity constructor is one greather than the maximum depth of its 
  arguments. The motivation for this is the \textit{small scope hypothesis}: if a 
  program is incorrect, it will almost allways fail on some small input \cite
  {andoni2003evaluating}. 

  In addition to SmallCheck, there is also \textit{Lazy} SmallCheck. In many cases, 
  the value of a property is determined only by part of the input. Additionally, 
  Haskell's lazy semantics allow for functions to be defined on partial inputs. The 
  prime example of this is a property \texttt{sorted :: Ord a => [a] -> Bool} that 
  returns \texttt{false} when presented with \texttt{1:0:$\bot$}. It is not necessary 
  to evaluate $\bot$ to determine that the input list is not ordered. 

  Partial values represent an entire class of values. That is, \texttt{1:0:$\bot$} can 
  be viewed as a representation of the set of lists that have prefix \texttt{[1, 0]}. 
  By checking properties on partial values, it is possible to falsify a property for 
  an entire class of values in one go, in some cases greatly reducing the amount of 
  testcases needed. 

\subsection{LeanCheck} 

  Where SmallCheck uses a value's \textit{depth} to bound the number of test values, 
  LeanCheck uses a value's \textit{size} \cite{matela2017tools}, where size is defined 
  as the number of construction applications of positive arity. Both SmallCheck and 
  LeanCheck contain functionality to enumerate functions similar to QuickCheck's 
  \texttt{Coarbitrary}. 

\subsection{Hegdgehog} 
  
  Hedgehog \cite{hedgehog} is a framework similar to QuickCheck, that aims to be a 
  more modern alternative. It includes support for monadic effects in generators and 
  concurrent checking of properties. Additionally it supports automatic schrinking for many datatypes. Unlike QuickCheck and SmallCheck, HedgeHog does not support (partial) automatic derivation of generators, but rather chooses to supply a comprehensive set of combinators, which the user can then use to assemble generators.

\subsection{Feat} 
  
  A downside to both SmallCheck and LeanCheck is that they do not provide an efficient 
  way to generate or sample large test values. QuickCheck has no problem with either, 
  but QuickCheck generators are often more tedious to write compared to their 
  SmallCheck counterpart. Feat \cite{duregaard2013feat} aims to fill this gap by 
  providing a way to efficiently enumerate algebraic types, employing memoization 
  techniques to efficiently find the $n^{th}$ element of an enumeration. 

\subsection{QuickChick: QuickCheck for Coq} 
  
  QuickChick is a QuickCheck clone for the proof assistant Coq \cite
  {denes2014quickchick}. The fact that Coq is a proof assistant enables the user to 
  reason about the testing framework itself \cite{paraskevopoulou2015foundational}. 
  This allows one, for example, to prove that generators adhere to some distribution. 

\subsection{QuickSpec: Automatic Generation of Specifications}

  A surprising application of property based testing is the automatic generation of 
  program specifications, proposed by Claessen et al. \cite{claessen2010quickspec} 
  with the tool \textit{QuickSpec}. QuickSpec automatically generates a set of 
  candidate formal specifications given a list of pure functions, specifically in the 
  form of algebraic equations. Random property based testing is then used to falsify 
  specifications. In the end, the user is presented with a set of equations for which 
  no counterexample was found.  

\section{Generating Constrained Test Data}\label{genconstrainedtd}

  Defining a suitable generation of test data for property based testing potentially very challenging, independent of whether we choose to sample from 
  or enumerate the space of test values. Writing generators for mutually recursive 
  datatypes with a suitable distribution is especially challenging. 
    
  We run into prolems when we desire to generate test data for properties with a 
  precondition. If a property's precondition is satisfied by few input values, it 
  becomes unpractical to test such a property by simply generating random input data, 
  and using rejection sampling to filter out those values that satisfy the desired 
  precondition. We will often end up with very few testcases, and we will end up with 
  a skewed distribution favoring those test values that have the largest probability 
  to be picked at random (often these are the simplest values that satisfy the 
  precondition). 
  
  The usual solution to this problem is to define a custom test data generator that 
  only produces data that satisfies the precondition. There are cases in which this is 
  not too difficult, however once we require more complex test data, such as 
  well-formed programs, this is quite a challenging task. 

\subsection{Lambda Terms} 

  A problem often considered in literature is the generation of (well-typed) lambda 
  terms \cite{palka2011testing, grygiel2013counting, claessen2015generating}. Good 
  generation of arbitrary program terms is especially interesting in the context of 
  testing compiler infrastructure, and lambda terms provide a natural first step 
  towards that goal. 

  Claessen and Duregaard \cite{claessen2015generating} adapt the techniques described 
  by Duregaard \cite{duregaard2013feat} to allow efficient generation of constrained 
  data. They use a variation on rejection sampling, where the space of values is 
  gradually refined by rejecting classes of values through partial evaluation (similar 
  to SmallCheck \cite{runciman2008smallcheck}) until a value satisfying the imposed 
  constrained is found. 

  An alternative approach centered around the semantics of the simply typed lambda 
  calculus is described by Pa{\l}ka et al. \cite{palka2011testing}. Contrary to the 
  work done by Claessen and Duregaard \cite{claessen2015generating}, where 
  typechecking is viewed as a black box, they utilize definition of the typing rules 
  to devise an algorithm for generation of random lambda terms. The basic approach is 
  to take some input type, and randomly select an inference rule from the set of rules 
  that could have been applied to arrive at the goal type. Obviously, such a procedure 
  does not guarantee termination, as repeated application of the function application 
  rule will lead to an arbitrarily large goal type. As such, the algorithm requires a 
  maximum search depth and backtracking in order to guarantee that a suitable term 
  will eventually be generated, though it is not guaranteed that such a term exists if 
  a bound on term size is enforced \cite{moczurad2000statistical}. 

  Wang \cite{wang2005generating} considers the problem of generating closed untyped 
  lambda terms. 

\subsection{Inductive Relations in Coq}

  An approach to generation of constrained test data for Coq's QuickChick was proposed 
  by Lampropoulos et al. \cite{lampropoulos2017generating} in their 2017 paper \textit
  {Generating Good Generators for Inductive Relations}. They observe a common pattern 
  where the required test data is of a simple type, but constrained by some 
  precondition. The precondition is then given by some inductive dependent relation 
  indexed by said simple type. The |Sorted| datatype shown in section \ref
  {introduction} is a good example of this

  They derive generators for such datatypes by abstracting over dependent inductive 
  relations indexed by simple types. For every constructor, the resulting type uses a 
  set of expressions as indices, that may depend on the constructor's arguments and 
  universally quantified variables. These expressions induce a set of unification 
  constraints that apply when using that particular constructor. These unification 
  constraints are then used when constructing generators to ensure that only values 
  for which the dependent inductive relation is inhabited are generated. 

\section{Generic Programming \& Type Universes}\label
{sec:lituniverses}

  Many type universes have been developed beyond those used in this thesis, some of 
  which are also designed to describe (a subset of) indexed datatypes. We describe a 
  few of them here, and briefly discuss how they relate to the universes we used. 

\subsection{SOP (Sum of Products)}\label{sop}

  On of the more simple representations is the so called \textit{Sum of Products} view 
  \cite{de2014true}, where datatypes are respresented as a choice between an arbitrary 
  amount of constructors, each of which can have any arity. This view corresponds to 
  how datatypes are defined in Haskell, and is closely related to the universe of 
  regular types. As we will see (for example in section \ref{patternfunctors}), other 
  universes too employ sum and product combinators to describe the structure of 
  datatypes, though they do not necessarily enforce the representation to be in 
  disjunctive normal form. Sum of Products, in its simplest form, cannot represent 
  mutually recursive families of datatypes. An extension that allows this has been 
  developed in \cite{miraldo2018sums}, and is available as a Haskell library through 
  \emph{Hackage}.  

\subsection{W-Types}\label{sec:wtypes}

  Introduced by Per Martin-Löf \cite{martin1984intuitionistic}, \emph{W-types} 
  abstract over tree-shaped data structures, such as natural numbers or binary trees. 
  W-types are defined by their \emph{shape} and \emph{position}, describing 
  respectively the set of constructors and the number of recursive positions. 

  Perhaps the best known definition of W-types is using an inductive datatype, with 
  one constructor taking a shape value, and a function from position to W-type: 

\includeagda{3}{winductive}

  However, we can use an alternate definition where we separate the universe into 
  codes, semantics and a fixpoint operation (listing \ref{lst:wtypes})

\includeagdalisting{3}{wtypes}{W-types defined with separate codes and semantics}
{lst:wtypes}

  We take this redundant step for two reasons: 

  \begin{enumerate}
    \item To unify the definition of W-types with the design pattern for type 
    universes we described in \cref{sec:tudesignpattern}. 

    \item To emphasize the the similarities between W-types, and the universe of 
    indexed containers, which will be further discussed in (TODO ref chapter 6)
  \end{enumerate}

  \begin{example} 

    Let us look at the natural numbers (listing \ref{lst:defnat}) as an example. We 
    can define the following W-type that is isomorphic to |ℕ|:

\includeagdanv{3}{wnat}

    The |ℕ| type has two constructors, hence our shape is a finite type with two 
    inhabitants (|Bool| in this case). We then map |false| to the empty type, 
    signifying that |zero| has no recursive subtrees, and |true| to the unit type, 
    denoting that |suc| has one recursive subtree. The isomorphism between |ℕ| and |Wℕ|
     is established in listing \ref{wnatiso}. 

  \end{example}

\includeagdalisting{3}{wnatiso}{Isomorphism between |ℕ| and |Wℕ|}{lst:wnatiso}

\subsection{Indexed Functors}

  Löh and Magalhães propose in their paper \emph{Generic Programming with Indexed 
  Functors} \cite{loh2011generic} a type universe for generic programming in Agda, 
  that is able to handle a large class of indexed datatypes. Their universe takes the 
  universe of regular types as a basis. 
  
  The semantics of the universe, however, is not a functor |Set → Set|, but rather an 
  \emph{indexed} functor |(I → Set) → O → Set|. Additionally, they add some 
  combinators, such as first order constructors to encode isomorphisms and fixpoints 
  as part of their universe. 

\subsection{Combinatorial Species}

  Combinatorial Species. Combinatorial species \cite{yorgey2010species} were 
  originally developed as a mathematical framework, but can also be used as an 
  alternative way of looking at datatypes. A species can, in terms of functional 
  programming, be thought of as a type constructor with one polymorphic argument. 
  Haskell’s ADTs (or regular types in general) can be described by definining familiar 
  combinators for species, such as sum and product.